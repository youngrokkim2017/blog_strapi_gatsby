{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blog_author_regex = \"<wp:author_login><!\\[CDATA\\[(.*?)\\]\\]>.*<wp:author_display_name><!\\[CDATA\\[(.*?)]]><\\/wp:author_display_name>.*<\\/wp:author>\"\n",
    "mag_author_regex = \"<!\\[CDATA\\[_issuem_author_name\\]\\]><\\/wp:meta_key>[\\s\\S]*?<wp:meta_value><!\\[CDATA\\[(.*?)\\]\\]\"\n",
    "\n",
    "media_regex = \"<item>[\\s\\S]*?<guid .*?>(.*?)<\\/guid>[\\s\\S]*?<wp:post_parent>(.*?)<\\/wp:post_parent>[\\s\\S]*?<\\/item>\"\n",
    "blog_regex = \"<item>[\\s\\S]*?<title>(.*?)<\\/title>[\\s\\S]*?<link>(.*?)<\\/link>[\\s\\S]*?<dc:creator><!\\[CDATA\\[(.*?)\\]\\]><\\/dc:creator>[\\s\\S]*?p=(.*?)<\\/guid>[\\s\\S]*?<content:encoded><!\\[CDATA\\[([\\s\\S]*?)\\]\\][\\s\\S]*?<wp:post_date><!\\[CDATA\\[(.*?)\\]\\]><\\/wp:post_date>[\\s\\S]*?<\\/item>\"\n",
    "mag_regex = '<item>[\\s\\S]*?<title>(.*?)<\\/title>[\\s\\S]*?<link>(.*?)<\\/link>[\\s\\S]*?<guid[\\s\\S]*?p=(.*?)<\\/guid>[\\s\\S]*?<content:encoded><!\\[CDATA\\[([\\s\\S]*?)\\]\\]><\\/content:encoded>[\\s\\S]*?<wp:post_date><!\\[CDATA\\[(.*?)\\]\\]><\\/wp:post_date>[\\s\\S]*?<category domain=\"issuem_issue\"[\\s\\S]*?<!\\[CDATA\\[(.*?)\\]\\][\\s\\S]*?<!\\[CDATA\\[_issuem_author_name\\]\\]><\\/wp:meta_key>[\\s\\S]*?<wp:meta_value><!\\[CDATA\\[(.*?)\\]\\][\\s\\S]*?<\\/item>'\n",
    "\n",
    "authors = {}\n",
    "\n",
    "with open('posts/authors.xml', 'r') as xml:\n",
    "    contents = xml.read()\n",
    "    for i, auth_display_name in re.findall(blog_author_regex, contents):\n",
    "        authors[i] = auth_display_name\n",
    "\n",
    "with open('posts/magazine-articles.xml', 'r') as xml:\n",
    "    contents = xml.read()\n",
    "    for auth_display_name in re.findall(mag_author_regex, contents):\n",
    "        authors[auth_display_name] = auth_display_name\n",
    "\n",
    "media = {}\n",
    "        \n",
    "with open('posts/media.xml', 'r') as xml:\n",
    "    contents = xml.read()\n",
    "    for url, parent_id in re.findall(media_regex, contents):\n",
    "        media[parent_id] = url        \n",
    "        \n",
    "authors_df = pd.DataFrame(authors.values()).replace('', np.nan).dropna()\n",
    "article_title_categories = pd.read_csv('posts/article-categories.csv')\n",
    "categories = pd.read_csv('posts/categories.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ] 100%       featured_image given not subscriptablend'"
     ]
    }
   ],
   "source": [
    "blog_posts = []\n",
    "\n",
    "with open('posts/export.xml', 'r') as xml:\n",
    "    contents = xml.read()\n",
    "    draw_progress_bar(0)\n",
    "    matches = re.findall(blog_regex, contents)\n",
    "    for i, post in enumerate(matches):\n",
    "        blog_posts.append({})\n",
    "        ID = post[3]\n",
    "\n",
    "        blog_posts[-1][\"id\"] = ID\n",
    "        blog_posts[-1][\"bsr_permalink\"] = post[1]\n",
    "        blog_posts[-1][\"title\"] = markdownify(post[0])\n",
    "        blog_posts[-1][\"author\"] = authors[post[2]]\n",
    "        \n",
    "        \n",
    "        if post[5]:\n",
    "            blog_posts[-1][\"created_at\"] = parser.parse(post[5]).strftime('%d %B %Y %I:%M%p')\n",
    "            \n",
    "        blog_posts[-1]['issue'] = np.nan\n",
    "        \n",
    "        if ID in media:\n",
    "            blog_posts[-1]['featured_image'] = media[ID]\n",
    "            flag = 'featured_image given'\n",
    "            \n",
    "        else:\n",
    "            blog_posts[-1]['featured_image'], flag = retrieve_img(post[1])\n",
    "            \n",
    "        newlines = re.sub('\\n', '<br><br>', post[4])\n",
    "        hlines = re.sub('<hr />', '-------', newlines)\n",
    "        out = re.sub('<!-- .*? -->', '', hlines)\n",
    "        final = re.sub(r'\\[caption .*?\\](.*?\\/>) (.*?)\\[\\/caption\\]', r'\\1<br><em>\\2</em>', out)\n",
    "\n",
    "        md = markdownify(final, heading_style=ATX).lstrip()\n",
    "        final = re.sub('\\n\\s*\\n', '<br><br>', md)\n",
    "#         final = re.sub(r'\\[caption .*?src=\"(.*?)\"[\\s\\S]*?\\[\\/caption\\]',r'<img src=\"\\1\"/>',final)\n",
    "#         print(final)\n",
    "        \n",
    "        blog_posts[-1][\"markdown\"] = markdownify(final, heading_style=ATX).lstrip()\n",
    "\n",
    "        draw_progress_bar(i/len(matches), flag)\n",
    "\n",
    "blog_posts = pd.DataFrame(blog_posts)\n",
    "blog_posts = blog_posts[~blog_posts['bsr_permalink'].str.contains(\"trashed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================= ] 100%       'NoneType' object is not subscriptablend'"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "\n",
    "with open('posts/magazine-articles.xml', 'r') as xml:\n",
    "    contents = xml.read() \n",
    "    draw_progress_bar(0)\n",
    "    matches = re.findall(mag_regex, contents)\n",
    "    for i, article in enumerate(matches):\n",
    "        articles.append({})\n",
    "        ID = article[2]\n",
    "\n",
    "        articles[-1][\"id\"] = ID\n",
    "        \n",
    "        articles[-1][\"bsr_permalink\"] = article[1]\n",
    "        articles[-1][\"title\"] = markdownify(article[0])\n",
    "        articles[-1][\"author\"] = authors[article[6]]\n",
    "        \n",
    "        if article[5]:\n",
    "            articles[-1][\"created_at\"] = parser.parse(article[4]).strftime('%d %B %Y %I:%M%p')\n",
    "\n",
    "        articles[-1]['issue'] = article[5]\n",
    "            \n",
    "        if ID in media:\n",
    "            articles[-1]['featured_image'] = media[ID]\n",
    "            flag = 'featured_image given'\n",
    "        else:\n",
    "            articles[-1]['featured_image'], flag = retrieve_img(article[1])\n",
    "            \n",
    "        newlines = re.sub('\\n', '<br><br>', article[3])\n",
    "        hlines = re.sub('<hr />', '-------', newlines)\n",
    "        out = re.sub('<!-- .*? -->', '', hlines)\n",
    "        final = re.sub(r'\\[caption .*?\\].*?(<img.*?\\/>).*?<\\/a> .*?(.*?)\\[\\/caption\\]', r'\\1<br><em>\\2</em>', out)\n",
    "        final = re.sub(r'\\[caption .*?\\](.*?\\/>) (.*?)\\[\\/caption\\]', r'\\1<br><em>\\2</em>', final)\n",
    "        \n",
    "\n",
    "        md = markdownify(final, heading_style=ATX).lstrip()\n",
    "        final = re.sub('\\n\\s*\\n', '<br><br>', md)\n",
    "        \n",
    "        articles[-1][\"markdown\"] = markdownify(final, heading_style=ATX).lstrip()\n",
    "\n",
    "        draw_progress_bar(i/len(matches), flag)\n",
    "\n",
    "articles = pd.DataFrame(articles)\n",
    "articles = articles[~articles['bsr_permalink'].str.contains(\"trashed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([blog_posts,articles],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=95, limit=1):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))    \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "    c = df_1['matches'].apply(lambda x: ', '.join([str(i[1]) for i in x]))\n",
    "    df_1['matches'] = m2\n",
    "    df_1['confidence'] = c\n",
    "\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_w_categories_fuzzy = fuzzy_merge(article_title_categories, merged, 'title', 'title', threshold=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_w_categories_fuzzy = articles_w_categories_fuzzy[articles_w_categories_fuzzy.confidence.astype(int)>95]\n",
    "fuzzy = articles_w_categories_fuzzy.drop(['title','confidence'], axis=1).rename(columns={\"matches\": \"title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_article_df = merged.merge(fuzzy, on='title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bsr_permalink</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>created_at</th>\n",
       "      <th>issue</th>\n",
       "      <th>featured_image</th>\n",
       "      <th>markdown</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>secondary_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "      <td>369</td>\n",
       "      <td>804</td>\n",
       "      <td>1127</td>\n",
       "      <td>214</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1125</td>\n",
       "      <td>1125</td>\n",
       "      <td>1095</td>\n",
       "      <td>276</td>\n",
       "      <td>1043</td>\n",
       "      <td>22</td>\n",
       "      <td>799</td>\n",
       "      <td>1116</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>16850</td>\n",
       "      <td>https://berkeleysciencereview.com/article/from...</td>\n",
       "      <td>From the Editor</td>\n",
       "      <td>Psych Your Mind</td>\n",
       "      <td>10 May 2020 09:03PM</td>\n",
       "      <td>Fall 2019</td>\n",
       "      <td>https://berkeleysciencereview.com/wp-content/u...</td>\n",
       "      <td></td>\n",
       "      <td>Life Science</td>\n",
       "      <td>Life Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                      bsr_permalink  \\\n",
       "count    1127                                               1127   \n",
       "unique   1125                                               1125   \n",
       "top     16850  https://berkeleysciencereview.com/article/from...   \n",
       "freq        2                                                  2   \n",
       "\n",
       "                  title           author           created_at      issue  \\\n",
       "count              1127             1127                 1127        369   \n",
       "unique             1095              276                 1043         22   \n",
       "top     From the Editor  Psych Your Mind  10 May 2020 09:03PM  Fall 2019   \n",
       "freq                 12              112                   17         24   \n",
       "\n",
       "                                           featured_image markdown  \\\n",
       "count                                                 804     1127   \n",
       "unique                                                799     1116   \n",
       "top     https://berkeleysciencereview.com/wp-content/u...            \n",
       "freq                                                    2        8   \n",
       "\n",
       "       primary_category secondary_category  \n",
       "count               214                 73  \n",
       "unique               11                 11  \n",
       "top        Life Science       Life Science  \n",
       "freq                 53                 25  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_article_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://berkeleysciencereview.com/article/from-the-editor-fall-2020/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.loc[367, 'bsr_permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_article_df.to_json('export-2020-12-26.js', orient='records', indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
